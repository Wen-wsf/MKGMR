{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新增提早停止\n",
    "目前沒有切train and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current CUDA device: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 CUDA 是否可用\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# 输出 CUDA 是否可用\n",
    "print(\"CUDA available:\", cuda_available)\n",
    "\n",
    "# 如果 CUDA 可用，打印当前 CUDA 设备的名称\n",
    "if cuda_available:\n",
    "    print(\"Current CUDA device:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items (movies): 131263\n",
      "Batch 0: User IDs: tensor([ 5137,   168, 11600,  8015,  2106,  9236,  3945,  1860,  3022,  5518,\n",
      "        10888,   799,  1936,  6034, 10440,  7771,  2091,  5683, 10333,  4283,\n",
      "        11733,  2928,  4490, 10431,  3343, 11656,  2695,  6770,  7391,  2802,\n",
      "         2631,  5601,  2964, 10721,  6456,  8063,  1323,  8598,  1175,  5796,\n",
      "         7016,  5857,  5762, 10428,  1355,  1253,  5115,  3761,  8420, 11995,\n",
      "         4043,  6339,   853, 10278,  2703,  8380, 12019,  4636,  4986,  8074,\n",
      "         6234,  6725, 11714,  8529,  4142,  3503, 10569,  9038,  1248,  9645,\n",
      "         9323,  7621, 11447, 11030,  9650,  8604,   386,  4169,  4525,   242,\n",
      "          935,    83, 11270,  6850,  9057,  2452,   746,  8049, 11493,  7896,\n",
      "         6433,  9183, 11057,  1150, 11445, 11047,  3566,  1726,  7230,   803,\n",
      "         1766,  9871,  7858, 10220,  8860,  1766,  7394, 11558,  4463, 10483,\n",
      "        11399,   941,  5714,    60,  3846,  4611,  2949,  3110,  8844, 11958,\n",
      "        12032,  1447,  7546,  5171,  5064,  8121,  1447,  9385]) Pos IDs: tensor([ 351,  106, 1195,  507, 3777, 2599, 1755,  798,   29,  758,  519,  872,\n",
      "         761, 1434, 3730,  413,  290, 2256,  497, 1635,  228, 3686, 1859,  604,\n",
      "         179,  910, 1551, 1364,   90, 2620,  252, 3688,  799,  999,    3,  228,\n",
      "        4379,  978, 3047, 2986,  418,  936, 1758,   16,  561,  683,  279,  587,\n",
      "        4482,  208,  587, 3697,  228,  753,  716,  390, 1358,  668,  758,  908,\n",
      "         346,  105, 4185, 1172,  252,  758, 1182, 2190,  357, 1755,  721, 1174,\n",
      "        2046, 2174, 1642, 1435, 1231,  264, 4065,  436,  415,  758,  198, 1485,\n",
      "         372,  649, 3450,  184,  793, 1434, 1031, 3205,  862, 1772, 1758, 4379,\n",
      "         208, 4409, 4065,  957,  758,  282, 1755, 3023, 1120, 4963, 5211,  193,\n",
      "         837,   97, 1894,  165,  208,  990,   90,  647, 1169,  179, 1788, 2342,\n",
      "         423, 3324,  272, 1174,  179,   59, 3047,   60]) Neg IDs: tensor([3831,  168,  253, 5195, 2745, 4174, 5677, 1278, 5224,  585, 4177, 4827,\n",
      "        5351, 5838, 5341, 3429, 5621, 4670,  163, 3192, 3922,  232, 2900, 2108,\n",
      "        1515, 5604, 4097, 2161,  884, 5394, 5019, 2538, 4816,   43, 4396, 1601,\n",
      "        1608, 3317, 5431, 4053, 1632, 3175, 3279, 5031, 3181, 1400,  900, 4052,\n",
      "        1175, 3066, 3520, 2227, 5662, 3628, 5617, 5032, 4918, 5351, 4913,  444,\n",
      "        1049,  362, 2501, 5189,  412,  786, 3148, 5051, 2331, 3833,  668, 3471,\n",
      "        3076, 3537,  623,  593, 3689, 3997,  335, 4275, 3378, 4494, 2596, 4703,\n",
      "        5203, 2666,  961, 4523, 3982, 2796, 4130, 5548,  350,  411,  307, 4703,\n",
      "        3290, 3395, 4276, 3352, 1001, 2686, 3592, 3574, 1663, 4101, 4634, 1600,\n",
      "         794, 3111, 5831, 3336, 2423, 1456, 1626, 3301, 4442, 5023, 2891, 4012,\n",
      "        4730, 2983, 5426, 3312, 5700, 2086, 4414, 2965])\n",
      "Batch 1: User IDs: tensor([ 1326,  6068,  7081,  3773,   408,  5692,  5857, 11379,  2620,  3791,\n",
      "        12036,   679,  9334, 11536,  1866,  1512, 10154,  6777, 11576,  9038,\n",
      "         6286,  8014,  6882, 10719,   932,    78,  7746,  2315,   459,  6548,\n",
      "         6104,  6497,  4518,  4604,  7406,  1601,  9284,  1052,  5596,  8874,\n",
      "         3363,   797,  1945, 10119,  1756,  8650, 10975,  2874,  4095,  1704,\n",
      "         1618,  5677,  8668,   688,  8761,  2756,  3283,   483,  5835,  8129,\n",
      "         5711,  8055,  9609,  7132,  9971,  2138,  7340, 11463, 11496,   758,\n",
      "         8823,  2977,  5044, 11678,  4193,  3512,  3060,  2166,  9077,  6689,\n",
      "         6068, 10337,  1644,  6539,  4734,  2159,   753, 11999,  5215,  1641,\n",
      "         5603, 11552,  7472,  7108,  3893,   754,  8243, 11116, 11678,  8147,\n",
      "         7874,  4549,  6294,  3261,  3081,  4306,  5891,  2831,  2306,   775,\n",
      "        11443,  5298,  4849,  9857,  6995, 11243,  8642,  5384,  1092,   104,\n",
      "         1467,  8497,  8875,  8595,   955,  1685,  8336,  2877]) Pos IDs: tensor([2787,  990,   73,  228, 2620,   70, 1686,   35, 3005,   90,  960, 3463,\n",
      "          73, 1358,   10,  282,  282,  355, 2670, 3223,  252, 1312,  132,  840,\n",
      "        2831, 3201,  355,  876, 3931, 2416,    9,  990,  575,  353, 3115, 2734,\n",
      "        1172, 1152,  519, 1367,  649,  587,  227,  184,  372, 2028, 1373,  118,\n",
      "         228, 2403,  956, 4226, 2427,  264, 3052, 2293, 2898,  649, 3463, 1652,\n",
      "         363, 1320, 4590, 1958, 1041, 4812,  161,  363,  118,  862,  505,  228,\n",
      "         184,  400,  122, 1172,  467, 4456, 1430, 1069, 1630, 3386, 4249, 4461,\n",
      "         228,   35,  531, 1320,  990, 4678,  208,  242, 4379, 1891, 1485,   90,\n",
      "           8, 1238,  590, 2716,  363, 2342, 2960,  425, 1660, 1755, 2291,  910,\n",
      "        1380, 1067,  118,  123, 3450, 1359,  208, 1081, 2233,  234, 2620,  660,\n",
      "        1752, 1709,  556,  709, 1554, 1518, 2333,  282]) Neg IDs: tensor([4516, 2414,  101, 4074, 4139, 2004, 3518, 4143, 4249, 1051,  716, 4114,\n",
      "        5832, 5728, 2729,  142, 5740, 3652, 5710, 1648, 1078, 2282, 5751, 5256,\n",
      "        4676, 4945,  122, 3652,  982, 3712, 3306, 5728, 1547, 3121, 2473, 1914,\n",
      "        3290, 1823, 5914, 3628, 5704,  218, 3683, 2204, 1520, 4446, 1527,  564,\n",
      "        3771, 2386,  730, 1830, 5033, 3594, 3220,  755, 1339, 4370,  480, 3665,\n",
      "        3246,  353, 1770, 1988, 4389, 4650, 2379,  942,  445,   72, 3103, 2576,\n",
      "        2005, 2198, 2022, 2120, 3545, 2971, 5060, 3594, 1184,  241, 1148,  993,\n",
      "        2998, 2927,  693, 4359, 2820, 2491, 1382, 3244, 1731,  361, 2294, 3999,\n",
      "         976, 1724, 3634, 1231, 4681, 2007, 3723, 5360, 3440, 3883, 2814, 5485,\n",
      "        1109, 5190, 1149, 1805, 2212, 3030,  600, 1099,  876, 2337, 3016, 1236,\n",
      "        4917, 5163, 3470,  233, 3916, 1104, 2663, 4330])\n",
      "Batch 2: User IDs: tensor([ 3572, 10755,  2212, 11768,   942, 10570,  5345,  2813,  8160,  5813,\n",
      "          313,  2446,  5287,  4891,  5503,  4223,  2932, 11276,  5685,  2847,\n",
      "        11495,  5813,  3047,  2296,  4633,  2479, 11011,  3711, 10160,  8793,\n",
      "        10058,  1099,  1518, 10359,  3305,  1388,  5595,  7922,  8251, 10806,\n",
      "         9620,  5851,  3658,  5704,  6333,   312,  8539,  6129,   201,  3992,\n",
      "        11422,  6068,  3081,  4455, 10540,  9631, 11502,  1703,  1002,  3167,\n",
      "         3728,  1626,  1901,  4783,  4278, 10536,  9038,  4263,  8429,  2669,\n",
      "         8103, 11156,  6657,  4281,  9262,  2082,  4400,  4198,  6936,  4382,\n",
      "        12052,  8993,  2379,  2276,  7782,  2277,  6459,  4237,  2600,  2144,\n",
      "         5800,  4810,   791, 11097,  8617,  4240,  5884,  9896,  2794,  8792,\n",
      "        11781, 11308,  7632,  5066,   650, 11563,  1051,  8374,  2637,  1626,\n",
      "         5035,  1454,    12,  2713,  5202,  4743,  3510,  9908,  1343,   462,\n",
      "         7135,  4922,  8328, 10993, 11618,  9936,  1154, 10285]) Pos IDs: tensor([ 558,  122,  514, 3701,  179,  339,  347, 1984, 2545, 1485, 1552, 1485,\n",
      "        3768, 2388, 2620,  980,  264, 3463, 1755,  961,  228, 1894, 1554, 2485,\n",
      "        4256, 2256, 4456, 1400, 1804,  155,  207,  105,  282, 4178, 1574, 4178,\n",
      "         302, 1367, 1774, 1635, 3201,  981,  372,  397,  179, 2234, 1575,  207,\n",
      "         527,  282,  990, 3445,  118, 1152,  381,  234, 3698, 1515, 3450, 3457,\n",
      "         355, 1604,  118, 3086,  977,  510,  519, 2132,  553,  981, 2485, 2716,\n",
      "         721,  758, 1070, 5076,  179, 1772,  758, 1312,  347, 1120, 1264,  397,\n",
      "        3535, 3753, 3232, 4841, 3086, 2136, 1712, 3697, 3047,  647, 1774,  234,\n",
      "         217, 1758, 2318,  649, 4482, 1696,  390,   80,  339, 2352, 2132,  112,\n",
      "         121, 1895, 4331,  910,  189,  337, 1630, 4332, 1186,  686, 1430, 1120,\n",
      "        1462,  436, 3721,  695, 4202, 3205,  960,  184]) Neg IDs: tensor([2418, 2546,  207, 3214,  185, 5293, 2619, 3510, 2918, 4409, 4856, 3741,\n",
      "        4874, 5407,  132, 2680,  194, 1801, 1228, 3278, 2778, 4827,  476, 3742,\n",
      "        1618,  338, 3651, 1405, 5215, 4767, 4644, 1557,   39, 3893, 3055, 3897,\n",
      "         869, 3154, 5550, 4934, 4330,  775, 5276, 1144, 2007, 1023, 2912, 1540,\n",
      "        2074, 2485,  212, 3254, 4425,  353, 1850, 1709, 5907, 4728, 4988, 5117,\n",
      "        2993, 3601, 3172, 4388, 5441, 5408,  273, 4384, 4330, 4875, 5544, 5152,\n",
      "         454, 5866, 1208, 5278, 2689, 2399, 3351, 3578, 1083, 3838, 5935, 4434,\n",
      "        1405, 5878,  629, 4371, 3147, 3923, 3073, 3714, 1646, 3122, 1829,  877,\n",
      "        2033, 1583,  950, 5769,  831,  158, 4908, 3555,  306, 4098,  782,  735,\n",
      "        5915, 3132, 2342, 1493, 2072, 1817, 4479, 4959,  962, 3809, 3125, 5519,\n",
      "         640, 2312,  207, 2368, 4854, 1402,   20, 1996])\n",
      "Batch 3: User IDs: tensor([ 6759,  9985,  8519,  7510,   943,  6176,  3859, 10877,  7889, 11102,\n",
      "        11284,  6737,  5585,  5426,   321,  7812, 10807,   803,  9985, 10484,\n",
      "         4611, 10221, 10933,   958,  5254,  1717,  6164,  7519, 10284,  1156,\n",
      "         6822, 11309, 11201,  8917,  7653,  4761,   282,  6770,  2144, 10622,\n",
      "        11832, 10862,  9373,  8601, 11244, 10721,  4966,  7158,  5716,  7956,\n",
      "         8910,   959,  6489, 10241, 10721,  8665,  7663,  1444,  4369,  4154,\n",
      "         6397, 10303,  9762, 10020, 11367,  2223,  3688,  4615,  1423,  1347,\n",
      "          104,  4788,  2061,  5749,  8601,   476, 10035,  8460, 11308,  2551,\n",
      "        10564, 11408,  5435,  4555,   207, 11308,  5462,  2338,  2904,  8116,\n",
      "          228,  6770,  4385,  2479, 11609,  1266, 10622,  7264,  5595, 10465,\n",
      "        11945,  1573,   814,  7609,  1059, 10460,  7117,  5129,  7331,  2285,\n",
      "        11014, 10222,  6765,  5324,  9651,  5901,  3164,  2551,  3989,  6068,\n",
      "         7609,  8751,  7227,  2337,  4563, 10417,  2742,  2677]) Pos IDs: tensor([ 279,  331, 3507,  798, 3046, 2487, 5076, 3348, 3629,   78, 1400,  282,\n",
      "        1756,  758, 1162, 4332, 1756, 2318,  575,  201, 3086, 1752,  282, 2485,\n",
      "         252, 1491,  290,  759,  758, 1488,  122,  134, 1312, 1231,  798, 2938,\n",
      "        1774, 1755,  721, 3081, 3753,   59, 1877, 3479, 2318,  560,   73,  131,\n",
      "        4332, 2486,   90, 1577,  228,  990, 4185,  187,  936,   78,  118,  636,\n",
      "         553, 1577, 1587, 4422, 1859,  519, 3892, 3697,  397, 2641,   99,  650,\n",
      "        2092,  519, 2621, 1587,  509, 1296, 3764, 1587, 1772,  179, 1970,  282,\n",
      "        2624, 3457,  141, 4010, 3056, 1340,  709, 1710,  156, 2356,  118, 2486,\n",
      "        1067,  162,  478, 1512, 1182,  412, 2599,  799,  601,  758,  507, 1755,\n",
      "        2492, 1663, 1903, 1772, 2132, 2233, 3223,  122, 1139, 3463, 3752, 2365,\n",
      "        1575, 4065, 2354, 2174,  587, 1172,  758, 1186]) Neg IDs: tensor([3163, 5694, 2967, 2469, 4599, 2433, 5249, 4172, 5738, 4837, 3827, 5706,\n",
      "        4446, 5136,  183, 1522, 4069, 3909, 2621, 5487, 4731, 5467, 5553, 4392,\n",
      "        5343, 4322, 1912, 5037, 2728, 2092, 5960, 3102,  847, 5576, 1862, 5904,\n",
      "        5525, 2963, 2684, 2775, 3890, 3076, 4383,  248, 1805, 4929,  976, 3396,\n",
      "        4894,  460, 3950, 2280, 5036, 3409, 1956, 2051, 3756, 5695, 3595, 4226,\n",
      "        3550, 2819, 4487, 1866, 5497, 4494,  113, 1911, 4556, 1877,  251, 5021,\n",
      "        3589, 2410, 3380,  535,  839, 3899,  649, 1074, 1259, 3543, 2782, 1618,\n",
      "         257, 5006, 3416, 3882, 5386, 1448, 2031, 1526, 4641, 5244,  448, 4608,\n",
      "         944, 2707, 4431, 3951, 5094, 5404, 5856, 4555, 5946,  476, 3977, 5989,\n",
      "        3304, 5233, 2764, 1424, 5787,  587, 2643, 1400, 5618, 5515, 1991, 3564,\n",
      "        2944, 2684, 4797, 4923, 2637, 5350, 2331, 1711])\n",
      "Batch 4: User IDs: tensor([ 1538,  5739,   944, 11576, 10326,  8131,  5729, 11793,  3743,  4592,\n",
      "         6728,   245, 11206,  5242,  4650,   296,  3924,  4023,  2082,  1506,\n",
      "         4849,  8396, 10280,  6957,  1538,  4813,  7404,  9881,  8379,  3572,\n",
      "         6626,  4969,  4059,  6602,  7965,  9249,  1878,  9372,  4295, 11950,\n",
      "         6861,  4767,  4358,  2206,  8396, 10444,  1059,  8492,  8625,   460,\n",
      "         4108,   242,  4265,  8523,  9479,  6254,  9679,  6800,  4197,   693,\n",
      "         2102, 11007,  2400,  6350,  4995,  5725,  6320,  7683,   619,   982,\n",
      "         4235, 10323,  2845, 10622,  2348,  2022,  8941,  1671,  4048,  1872,\n",
      "        12009,  5317,  9188,  4631,  5818, 10304,  8089,  9220,  9936, 10580,\n",
      "          900,   205,  7610,  1618,  3512,  3115, 11914,  4556,  9038,  3297,\n",
      "         3366, 11115,  9682,  8829,   350,  1733, 12036,  9138,  5141,  1093,\n",
      "         4563,  1470,  2461,  4064,  7975, 10219,     4, 11162,  1273,  9365,\n",
      "         2595,  5339,  7899,  4183,  6599,  4698,  3061,  9922]) Pos IDs: tensor([2415, 1642, 4428,  961, 1182,  360,  165,  378, 1769, 1356,  489, 1358,\n",
      "        1402, 4451,   90,  554, 3450,  228, 3535,  825, 3592,  660,   87,  510,\n",
      "        1322,  862, 1485,  141, 1577, 1968, 1048,  278,  372,   97,  807, 2620,\n",
      "         516,  343,  961,  756, 4401,   54, 4957,  282,   77, 1755, 1752, 3561,\n",
      "         390, 1214, 1582, 1454,  179, 1040, 1040,  295,  234, 4963, 1120,  761,\n",
      "        5171, 1430, 2861, 1712,   77, 1262,  519,  531, 3696,  118,  195, 2599,\n",
      "        1267, 1034,   58,  102, 2620, 1266,  961, 1356, 2218,  986,  962, 5287,\n",
      "        3670,  234, 4008, 3777, 3362,  347, 4957, 2365, 1208, 1877,  682,  282,\n",
      "        2256,  122, 5404, 1580,  656, 2599, 1252, 2407,  916,  957,  758,  527,\n",
      "         497,  210, 3674,  123, 3752, 1210, 2256, 3463, 3047, 2145,  906, 1811,\n",
      "          88, 3592,  364,  300, 1172,    1,  911,    1]) Neg IDs: tensor([ 101, 5460, 3811, 1761, 2947, 5783, 4597, 4177, 4344, 1241, 4732, 3494,\n",
      "         334, 5228, 3591, 3654, 4737, 5073, 4451, 3367, 2062, 2433, 2574,  667,\n",
      "        1875, 3366,  921, 5027, 3613,  564, 4793, 4206, 5948, 3123,  543, 2525,\n",
      "        3724, 2818, 2745, 2232, 2981, 5273, 2798,  856, 1605, 2196,  233, 2371,\n",
      "        3888, 4224, 4153, 3220, 1436, 4227, 1319, 5650, 3574, 3798, 2887, 1633,\n",
      "        5791, 5993, 3225, 1568, 4614,  751, 3669, 3314, 5768, 1034, 4859, 3253,\n",
      "        5582, 3449, 1238, 2206, 3519, 5722, 3780, 3908, 3872,  552, 4539, 4943,\n",
      "        5345,  332, 3548, 2606,  275, 2354, 5097, 1815, 5491, 5883, 3381,  154,\n",
      "        3985, 2886, 4498, 4942, 5059, 1200, 4464,  875, 5466,  459, 4846, 3924,\n",
      "        1137, 5867, 2041,  898, 2376, 3448,  703, 2548,  516,  146, 3630, 5790,\n",
      "        2132, 3783, 1874, 2381, 3604,  190, 3275,  811])\n",
      "Batch 5: User IDs: tensor([ 3766,  9676,  9441,  3634,  3105,  2772,  2631,  8135,  3646,  6096,\n",
      "         8045,  4102,  3725,  9486,  4054,  4248, 12048,  6489,  4188,  8254,\n",
      "         6009,  2100,  8800, 10961,  7144,  5043,  6269,  2276,  8254,  1644,\n",
      "          862,  8510,  1852,  8049,  5158,  8158,  7829,  5345,  5137,  2972,\n",
      "         2425, 10259,   304,  6246,  9829,   399,  8648,  4978,  2588,  2088,\n",
      "         5845,   554,  4455,  1447,  4455,  9587,  3567, 11716,  1665, 10396,\n",
      "        11109, 11657,  1058,  2150,   485,  6944,  4563,  6281,  2756,  3744,\n",
      "         5125,  3859,  8502,  3851,  9270,  6255,  3745,  3127,  4197,  7178,\n",
      "         1152,  7878,  2442, 11645,  9587,   983,  6729,  6770,   612,  2914,\n",
      "        11367,  4249,  8564,  6575, 11944,  1104,  1573,  7169,  7549,  5426,\n",
      "         2118,  2250,  6489,  3087, 10102,  3719,  4608, 11455,  8391,  1447,\n",
      "         5262,  6979,  5199, 11455,  4295,  2586,  1201, 11807,  6330,  7816,\n",
      "         6867,  4108,  4316, 10907,   116, 11361,  4286, 10102]) Pos IDs: tensor([  23,  577,  519, 2007, 3056, 1133, 1769, 1152,  684, 2355,   50, 1213,\n",
      "         961,  468, 1827,  208, 1208,  264, 3696, 2027,  721,   21,  228,  355,\n",
      "        5512, 1041,  378, 4422, 1984, 3752,  375,  601,  406, 4359,   90,  392,\n",
      "        1162,  561,  738,  721,  351,  123,  337,  758, 1859, 2458, 3715, 1041,\n",
      "        2450,  372, 3450, 1069, 1320,  282, 1514,  910, 1322, 4010,  228,  793,\n",
      "         289,  208,  647,  155, 1485, 5057,  508,   29, 2123, 1215, 1120,  761,\n",
      "        3344, 3185,  587, 3752, 2416, 2486, 1551,  421, 2415,  355,   32, 2599,\n",
      "        3333, 1133,  756,  761, 1758,  327,  282,  181, 2458,  118,  553,  201,\n",
      "        1004, 2132,  208,  519,  234, 2620,    9,  423, 1554, 2174, 1774, 1575,\n",
      "         207,  496,  721, 4113,  674, 1599,  404,  378, 1755, 2132,  118,  980,\n",
      "         123, 1367, 3636,  102,  527,  649, 3165,  100]) Neg IDs: tensor([1072, 4384, 5962, 1907, 3140, 5827, 4041, 1895, 1908, 4142,  589, 5511,\n",
      "         377, 4262, 4033, 5200,  855, 3711, 4778, 1141, 2015, 3034, 2944, 3579,\n",
      "        5748, 1477, 2747, 2106, 5502, 4428, 5772, 2891, 5756, 3775, 5217, 3432,\n",
      "         758, 1842, 2549, 1326,   99, 3048, 3877, 1102, 5954, 2806, 3405, 3673,\n",
      "        1732, 1257, 1480, 1561, 5724, 2805, 5298, 3918, 2409, 1178, 1879, 2973,\n",
      "        1358, 5054, 2185, 3859, 5352, 4974, 1172, 3123, 4486, 2165, 5975, 1288,\n",
      "         740, 2637, 1681, 4721,  375, 1650, 3370, 1478, 1841,  701, 3989, 2114,\n",
      "        3769, 1826, 3655, 3787, 5308, 5933, 3085, 3087, 4298, 5818, 1659,  153,\n",
      "        5151, 2050, 3594, 1540, 5193, 3756, 2315, 2391, 3780, 4031,  613, 4078,\n",
      "        5447, 2903, 3578, 3522, 2880, 3107, 4162, 3366,  202, 3755, 2421,  179,\n",
      "        4162, 4299, 3981, 4069, 3013,  501,  115,  599])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 数据集类定义\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, user_item_pairs, total_items):\n",
    "        self.user_item_pairs = user_item_pairs\n",
    "        self.total_items = total_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_item_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, pos_item = self.user_item_pairs[idx]\n",
    "        neg_item = np.random.randint(0, self.total_items)\n",
    "        while neg_item == pos_item:\n",
    "            neg_item = np.random.randint(0, self.total_items)\n",
    "        return user, pos_item, neg_item\n",
    "# Path to your saved graph\n",
    "graph_path = r'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\mainmodel\\graph\\hetero_graph03_with_V_T_A_features.pkl'\n",
    "\n",
    "# Load the graph\n",
    "with open(graph_path, 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "# 数据路径\n",
    "data_path = 'D:\\\\CODE\\\\multi-model knowledge graph multi-graph recommendation system\\\\data\\\\cleanuser_rating.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "total_items = df['movieId'].max() + 1  # 确保这是正确的电影总数\n",
    "print(\"Total items (movies):\", total_items)\n",
    "# 将用户ID和电影ID编码为连续整数\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df['userId'] = user_encoder.fit_transform(df['userId'])\n",
    "df['movieId'] = item_encoder.fit_transform(df['movieId'])\n",
    "num_users = df['userId'].nunique()  # 计算唯一的用户数量\n",
    "num_items = df['movieId'].nunique()  # 计算唯一的电影数量\n",
    "# 分割数据为训练集和验证集\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为用户-项目配对\n",
    "train_pairs = list(zip(train_df['userId'].values, train_df['movieId'].values))\n",
    "valid_pairs = list(zip(valid_df['userId'].values, valid_df['movieId'].values))\n",
    "\n",
    "# 总电影数\n",
    "total_items = df['movieId'].max() + 1\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = BPRDataset(train_pairs, total_items)\n",
    "valid_dataset = BPRDataset(valid_pairs, total_items)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "try:\n",
    "    for i, (user_ids, pos_item_ids, neg_item_ids) in enumerate(train_loader):\n",
    "        print(f\"Batch {i}: User IDs: {user_ids} Pos IDs: {pos_item_ids} Neg IDs: {neg_item_ids}\")\n",
    "        if i >= 5:  # 只打印前5批次以限制输出\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are available in 'movietext' nodes.\n",
      "Shape of the features: torch.Size([5996, 384])\n",
      "Average of features: 0.0\n",
      "Non-zero elements in features: 0\n",
      "Incorrect dimension of features for movietext: 384\n",
      "Features are available in 'movieimage' nodes.\n",
      "Shape of the features: torch.Size([5996, 2048])\n",
      "Average of features: 0.25277090072631836\n",
      "Non-zero elements in features: 8816537\n",
      "Features are available in 'movieaudio' nodes.\n",
      "Shape of the features: torch.Size([5996, 128])\n",
      "Average of features: 0.0\n",
      "Non-zero elements in features: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import dgl\n",
    "\n",
    "# Load your graph\n",
    "with open('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\mainmodel\\graph\\hetero_graph03_with_V_T_A_features.pkl', 'rb') as f:\n",
    "    hetero_graph = pickle.load(f)\n",
    "\n",
    "# Function to check features for a specific node type\n",
    "def check_features(graph, node_type):\n",
    "    if node_type in graph.ntypes:\n",
    "        if 'features' in graph.nodes[node_type].data:\n",
    "            features = graph.nodes[node_type].data['features']\n",
    "            print(f\"Features are available in '{node_type}' nodes.\")\n",
    "            print(f\"Shape of the features: {features.shape}\")\n",
    "            print(f\"Average of features: {torch.mean(features)}\")\n",
    "            print(f\"Non-zero elements in features: {torch.count_nonzero(features)}\")\n",
    "            if node_type == 'movietext' and features.shape[1] != 128:  # Adjusted expected dimension\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "            elif node_type == 'movieimage' and features.shape[1] != 2048:  # Expected dimension for image\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "            elif node_type == 'movieaudio' and features.shape[1] != 128:  # Expected dimension for audio\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "        else:\n",
    "            print(f\"No features data found in '{node_type}' nodes.\")\n",
    "    else:\n",
    "        print(f\"No '{node_type}' node type found in the graph.\")\n",
    "\n",
    "# Check features for each modality\n",
    "check_features(hetero_graph, 'movietext')\n",
    "check_features(hetero_graph, 'movieimage')\n",
    "check_features(hetero_graph, 'movieaudio')\n",
    "\n",
    "# Optionally, save the graph again if modifications are made\n",
    "# with open('hetero_graph_with_updated_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(hetero_graph, f)\n",
    "\n",
    "# If you need to reload and check the graph, uncomment these lines\n",
    "# with open('hetero_graph_with_updated_features.pkl', 'rb') as f:\n",
    "#     loaded_graph = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv, GATConv\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "import dgl\n",
    "\n",
    "class HeteroGraphConv(nn.Module):\n",
    "    def __init__(self, num_users, num_items, feature_sizes, embedding_dim=128, id_embedding_size=16, num_heads=4, num_layers=4, dropout=0.5):\n",
    "        super(HeteroGraphConv, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        # Embedding layers for users and items\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # Embedding layers for ID-based nodes, including production team members\n",
    "        self.production_team_nodes = [\n",
    "            'actor', 'actress', 'composer', 'director', 'editor', \n",
    "            'producer', 'writer'\n",
    "        ]\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            node_type: nn.Embedding(10000, id_embedding_size) for node_type in feature_sizes if node_type not in ['movie', 'movieimage', 'movietext', 'movieaudio']\n",
    "        })\n",
    "\n",
    "        # Transformation layers for modal-specific features\n",
    "        self.image_transform = nn.Linear(feature_sizes['movieimage'], 32)\n",
    "        self.text_transform = nn.Linear(feature_sizes['movietext'], 32)\n",
    "        self.audio_transform = nn.Linear(feature_sizes['movieaudio'], 32)\n",
    "\n",
    "        # GAT layers for each modality\n",
    "        self.gat_layers = nn.ModuleDict({\n",
    "            'movieimage': nn.ModuleList([GATConv(32, 32, num_heads=num_heads) for _ in range(num_layers)]),\n",
    "            'movietext': nn.ModuleList([GATConv(32, 32, num_heads=num_heads) for _ in range(num_layers)]),\n",
    "            'movieaudio': nn.ModuleList([GATConv(32, 32, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        })\n",
    "\n",
    "        # GAT layers for production team members\n",
    "        for node in self.production_team_nodes:\n",
    "            self.gat_layers[node] = nn.ModuleList([GATConv(id_embedding_size, 32, num_heads=num_heads) for _ in range(num_layers)])\n",
    "\n",
    "        # Attention layer for feature aggregation\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=32 * num_heads, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        # Additional layer to process concatenated features\n",
    "        self.fc = nn.Linear(32 * (3 + len(self.production_team_nodes)) * num_heads, 128)  # Adjust the size according to the output of GAT\n",
    "\n",
    "    def forward(self, user_ids, item_ids, graph=None, features=None):\n",
    "        user_emb = self.user_embeddings(user_ids)\n",
    "        item_emb = self.item_embeddings(item_ids)\n",
    "\n",
    "        if graph is not None and features is not None:\n",
    "            # Process modal-specific features if provided\n",
    "            image_features = F.relu(self.image_transform(features['movieimage']))\n",
    "            text_features = F.relu(self.text_transform(features['movietext']))\n",
    "            audio_features = F.relu(self.audio_transform(features['movieaudio']))\n",
    "\n",
    "            for layer in self.gat_layers['movieimage']:\n",
    "                image_features = layer(graph, image_features).flatten(1)\n",
    "                image_features = F.dropout(image_features, self.dropout, training=self.training)\n",
    "            for layer in self.gat_layers['movietext']:\n",
    "                text_features = layer(graph, text_features).flatten(1)\n",
    "                text_features = F.dropout(text_features, self.dropout, training=self.training)\n",
    "            for layer in self.gat_layers['movieaudio']:\n",
    "                audio_features = layer(graph, audio_features).flatten(1)\n",
    "                audio_features = F.dropout(audio_features, self.dropout, training=self.training)\n",
    "\n",
    "            # Process production team members' features\n",
    "            production_features = []\n",
    "            for node_type in self.production_team_nodes:\n",
    "                team_features = self.embeddings[node_type](graph.nodes[node_type].data['feat'])\n",
    "                for layer in self.gat_layers[node_type]:\n",
    "                    team_features = layer(graph, team_features).flatten(1)\n",
    "                    team_features = F.dropout(team_features, self.dropout, training=self.training)\n",
    "                production_features.append(team_features)\n",
    "\n",
    "            # Concatenate all features\n",
    "            combined_features = torch.cat([image_features, text_features, audio_features] + production_features, dim=1)\n",
    "            combined_features = F.relu(self.fc(combined_features))\n",
    "            combined_features = F.dropout(combined_features, self.dropout, training=self.training)\n",
    "\n",
    "            # Apply attention mechanism\n",
    "            combined_features = combined_features.unsqueeze(0)\n",
    "            attn_output, _ = self.attention(combined_features, combined_features, combined_features)\n",
    "            combined_features = attn_output.squeeze(0)\n",
    "\n",
    "            # Symmetric normalization\n",
    "            user_deg = graph.in_degrees(user_ids).float().clamp(min=1)\n",
    "            item_deg = graph.in_degrees(item_ids).float().clamp(min=1)\n",
    "            norm = torch.sqrt(user_deg.unsqueeze(1) * item_deg.unsqueeze(1))\n",
    "\n",
    "            combined_features = combined_features / norm\n",
    "\n",
    "            return user_emb, item_emb, combined_features\n",
    "        return user_emb, item_emb, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(users, pos_items, neg_items, lambda_reg, model):\n",
    "    # 獲取用戶和項目的嵌入\n",
    "    user_embeddings = model.embedding_user(users)\n",
    "    pos_item_embeddings = model.embedding_item(pos_items)\n",
    "    neg_item_embeddings = model.embedding_item(neg_items)\n",
    "    \n",
    "    # 計算用戶對正樣本和負樣本的偏好預測\n",
    "    pos_scores = torch.sum(user_embeddings * pos_item_embeddings, dim=1)\n",
    "    neg_scores = torch.sum(user_embeddings * neg_item_embeddings, dim=1)\n",
    "    \n",
    "    # 計算 BPR 損失\n",
    "    loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "    \n",
    "    # 添加 L2 正則化\n",
    "    reg_loss = lambda_reg * (user_embeddings.norm(2).pow(2) + \n",
    "                             pos_item_embeddings.norm(2).pow(2) +\n",
    "                             neg_item_embeddings.norm(2).pow(2))\n",
    "    \n",
    "    return loss + reg_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dcg_at_k(scores, k=5):\n",
    "    ranks = torch.log2(torch.arange(2, k+2).float()).to(scores.device)  # Log term in DCG formula\n",
    "    return (scores[:k] / ranks).sum()  # Only consider the top k scores\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_scores, true_relevance, k=5):\n",
    "    _, indices = torch.sort(predicted_scores, descending=True)\n",
    "    true_sorted_by_pred = true_relevance[indices]\n",
    "    ideal_sorted, _ = torch.sort(true_relevance, descending=True)\n",
    "\n",
    "    dcg = dcg_at_k(true_sorted_by_pred[:k])\n",
    "    idcg = dcg_at_k(ideal_sorted[:k])\n",
    "    return (dcg / idcg).item() if idcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之後要改成ndcg的版本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# import torch\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, epochs, lambda_reg, optimizer, validation_loader=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "\n",
    "#         # Train with BPR loss\n",
    "#         for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "#             _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "            \n",
    "#             # BPR Loss\n",
    "#             pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "#             neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "#             loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "            \n",
    "#             reg_loss = lambda_reg * (user_embeddings.norm(2).pow(2) + \n",
    "#                                      pos_item_embeddings.norm(2).pow(2) +\n",
    "#                                      neg_item_embeddings.norm(2).pow(2))\n",
    "#             total_loss_val = loss + reg_loss\n",
    "#             total_loss_val.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += total_loss_val.item()\n",
    "\n",
    "#         print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "#         # Evaluate using NDCG\n",
    "#         if validation_loader:\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 all_predicted_scores = torch.Tensor()\n",
    "#                 all_true_scores = torch.Tensor()\n",
    "#                 for user_ids, pos_item_ids, neg_item_ids in validation_loader:\n",
    "#                     user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "#                     _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "                    \n",
    "#                     pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "#                     neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "                    \n",
    "#                     scores = pos_scores - neg_scores\n",
    "#                     true_relevance = torch.ones_like(scores)  # assuming all positive items should be ranked higher\n",
    "                    \n",
    "#                     all_predicted_scores = torch.cat((all_predicted_scores, scores), 0)\n",
    "#                     all_true_scores = torch.cat((all_true_scores, true_relevance), 0)\n",
    "                \n",
    "#                 ndcg_value = ndcg_at_k(all_predicted_scores, all_true_scores, k=5)\n",
    "#                 print(f'Epoch {epoch+1}: Validation NDCG@5 = {ndcg_value:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, epochs, lambda_reg, optimizer, device, validation_loader=None):\n",
    "#     model.to(device)  # 确保模型在正确的设备上\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "\n",
    "#         for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "#             user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "#             _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "            \n",
    "#             # BPR Loss\n",
    "#             pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "#             neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "#             loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "            \n",
    "#             reg_loss = lambda_reg * (user_embeddings.norm(2).pow(2) +\n",
    "#                                      pos_item_embeddings.norm(2).pow(2) +\n",
    "#                                      neg_item_embeddings.norm(2).pow(2))\n",
    "#             total_loss_val = loss + reg_loss\n",
    "#             total_loss_val.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += total_loss_val.item()\n",
    "\n",
    "#         print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "#         if validation_loader:\n",
    "#             evaluate_metrics(model, validation_loader)\n",
    "# def evaluate_metrics(model, validation_loader, k=5):\n",
    "#     device = next(model.parameters()).device  # 获取模型参数所在的设备\n",
    "#     model.eval()\n",
    "#     total_dcg = 0\n",
    "#     total_ndcg = 0\n",
    "#     total_recall = 0\n",
    "#     total_relevant_items = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in validation_loader:\n",
    "#             user_ids, pos_item_ids, neg_item_ids = data\n",
    "#             user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "#             user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "#             _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "            \n",
    "#             pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "#             neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            \n",
    "#             scores = torch.cat((pos_scores, neg_scores))\n",
    "#             labels = torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores)))  # 真实标签\n",
    "            \n",
    "#             # 计算分数并排序\n",
    "#             _, indices = torch.sort(scores, descending=True)\n",
    "#             sorted_labels = labels[indices]\n",
    "            \n",
    "#             # 使用已定义的函数计算 DCG 和 NDCG\n",
    "#             dcg_value = dcg_at_k(sorted_labels, k)\n",
    "#             idcg_value = dcg_at_k(torch.ones(k).to(device), k)  # 理想情况下的排序\n",
    "#             ndcg_value = dcg_value / max(idcg_value, 1e-10)  # 避免除以零\n",
    "#             recall_value = sorted_labels[:k].sum() / labels.sum()\n",
    "\n",
    "#             total_dcg += dcg_value\n",
    "#             total_ndcg += ndcg_value\n",
    "#             total_recall += recall_value\n",
    "#             total_relevant_items += labels.sum()\n",
    "\n",
    "#     average_dcg = total_dcg / len(validation_loader)\n",
    "#     average_ndcg = total_ndcg / len(validation_loader)\n",
    "#     average_recall = total_recall / len(validation_loader)\n",
    "#     print(f'Average DCG@{k}: {average_dcg:.4f}')\n",
    "#     print(f'Average NDCG@{k}: {average_ndcg:.4f}')\n",
    "#     print(f'Average Recall@{k}: {average_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\GNN\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 299.9722746115699\n",
      "Average DCG@5: 0.3842\n",
      "Average NDCG@5: 0.1303\n",
      "Average Recall@5: 0.0054\n",
      "Validation Loss: 2.3830\n",
      "Epoch 2: Average Training Loss: 116.72475050437023\n",
      "Average DCG@5: 0.1888\n",
      "Average NDCG@5: 0.0640\n",
      "Average Recall@5: 0.0026\n",
      "Validation Loss: 1.1668\n",
      "Epoch 3: Average Training Loss: 50.982290731324056\n",
      "Average DCG@5: 0.1439\n",
      "Average NDCG@5: 0.0488\n",
      "Average Recall@5: 0.0021\n",
      "Validation Loss: 0.7992\n",
      "Epoch 4: Average Training Loss: 24.057049934413843\n",
      "Average DCG@5: 0.1199\n",
      "Average NDCG@5: 0.0407\n",
      "Average Recall@5: 0.0016\n",
      "Validation Loss: 0.7128\n",
      "Epoch 5: Average Training Loss: 12.522783073256589\n",
      "Average DCG@5: 0.1139\n",
      "Average NDCG@5: 0.0386\n",
      "Average Recall@5: 0.0017\n",
      "Validation Loss: 0.6959\n",
      "Epoch 6: Average Training Loss: 7.224949376403436\n",
      "Average DCG@5: 0.4017\n",
      "Average NDCG@5: 0.1363\n",
      "Average Recall@5: 0.0064\n",
      "Validation Loss: 0.6935\n",
      "Epoch 7: Average Training Loss: 4.539979433992556\n",
      "Average DCG@5: 0.8050\n",
      "Average NDCG@5: 0.2730\n",
      "Average Recall@5: 0.0121\n",
      "Validation Loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroGraphConv(\n",
       "  (user_embeddings): Embedding(12171, 128)\n",
       "  (item_embeddings): Embedding(5996, 128)\n",
       "  (embeddings): ModuleDict(\n",
       "    (actor): Embedding(10000, 16)\n",
       "    (actress): Embedding(10000, 16)\n",
       "    (director): Embedding(10000, 16)\n",
       "    (producer): Embedding(10000, 16)\n",
       "    (user): Embedding(10000, 16)\n",
       "  )\n",
       "  (image_transform): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  (text_transform): Linear(in_features=384, out_features=32, bias=True)\n",
       "  (audio_transform): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (gat_layers): ModuleDict(\n",
       "    (movieimage): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (movietext): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (movieaudio): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (actor): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (actress): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (composer): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (director): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (editor): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (producer): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (writer): ModuleList(\n",
       "      (0-3): 4 x GATConv(\n",
       "        (fc): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1280, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, epochs, lambda_reg, optimizer, device, validation_loader=None, patience=5):\n",
    "    model.to(device)  # 确保模型在正确的设备上\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            user_embeddings, pos_item_embeddings, _ = model(user_ids, pos_item_ids)\n",
    "            _, neg_item_embeddings, _ = model(user_ids, neg_item_ids)\n",
    "            \n",
    "            # BPR Loss\n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "            \n",
    "            reg_loss = lambda_reg * (user_embeddings.norm(2).pow(2) +\n",
    "                                     pos_item_embeddings.norm(2).pow(2) +\n",
    "                                     neg_item_embeddings.norm(2).pow(2))\n",
    "            total_loss_val = loss + reg_loss\n",
    "            total_loss_val.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += total_loss_val.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        if validation_loader:\n",
    "            val_loss = evaluate_metrics(model, validation_loader)\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "\n",
    "def evaluate_metrics(model, validation_loader, k=5):\n",
    "    device = next(model.parameters()).device  # 获取模型参数所在的设备\n",
    "    model.eval()\n",
    "    total_dcg = 0\n",
    "    total_ndcg = 0\n",
    "    total_recall = 0\n",
    "    total_relevant_items = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = data\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            user_embeddings, pos_item_embeddings, _ = model(user_ids, pos_item_ids)\n",
    "            _, neg_item_embeddings, _ = model(user_ids, neg_item_ids)\n",
    "            \n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            \n",
    "            scores = torch.cat((pos_scores, neg_scores))\n",
    "            labels = torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores))).to(device)  # 真实标签\n",
    "            \n",
    "            # 计算验证损失\n",
    "            val_loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # 计算分数并排序\n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            sorted_labels = labels[indices]\n",
    "            \n",
    "            # 使用已定义的函数计算 DCG 和 NDCG\n",
    "            dcg_value = dcg_at_k(sorted_labels, k)\n",
    "            idcg_value = dcg_at_k(torch.ones(k).to(device), k)  # 理想情况下的排序\n",
    "            ndcg_value = dcg_value / max(idcg_value, 1e-10)  # 避免除以零\n",
    "            recall_value = sorted_labels[:k].sum() / labels.sum()\n",
    "\n",
    "            total_dcg += dcg_value\n",
    "            total_ndcg += ndcg_value\n",
    "            total_recall += recall_value\n",
    "            total_relevant_items += labels.sum()\n",
    "\n",
    "    average_dcg = total_dcg / len(validation_loader)\n",
    "    average_ndcg = total_ndcg / len(validation_loader)\n",
    "    average_recall = total_recall / len(validation_loader)\n",
    "    average_val_loss = total_val_loss / len(validation_loader)\n",
    "    print(f'Average DCG@{k}: {average_dcg:.4f}')\n",
    "    print(f'Average NDCG@{k}: {average_ndcg:.4f}')\n",
    "    print(f'Average Recall@{k}: {average_recall:.4f}')\n",
    "    print(f'Validation Loss: {average_val_loss:.4f}')\n",
    "    return average_val_loss\n",
    "\n",
    "def dcg_at_k(sorted_labels, k):\n",
    "    sorted_labels = sorted_labels[:k]\n",
    "    gains = 2 ** sorted_labels - 1\n",
    "    discounts = torch.log2(torch.arange(2, k + 2).float().to(sorted_labels.device))  # 将discounts放到相同设备上\n",
    "    return (gains / discounts).sum()\n",
    "\n",
    "def dcg_at_k(sorted_labels, k):\n",
    "    sorted_labels = sorted_labels[:k]\n",
    "    gains = 2 ** sorted_labels - 1\n",
    "    discounts = torch.log2(torch.arange(2, k + 2).float().to(sorted_labels.device))  # 将discounts放到相同设备上\n",
    "    return (gains / discounts).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_sizes = {\n",
    "    'movie': 2560,    # 如果电影节点存储所有多媒体特征的总和\n",
    "    'actor': 50,      # 演员的嵌入维度\n",
    "    'actress': 50,    # 女演员的嵌入维度\n",
    "    'director': 50,   # 导演的嵌入维度\n",
    "    'producer': 50,   # 制片的嵌入维度\n",
    "    'movieimage': 2048,  # 图像特征维度\n",
    "    'movietext': 384,    # 文字特征维度\n",
    "    'movieaudio': 128,   # 音频特征维度\n",
    "    'user': 100         # 假设用戶的嵌入维度，可能需要根据用户数量和系统复杂性进行调整\n",
    "}\n",
    "# 初始化和配置模型，加载数据，然后开始训练和评估\n",
    "model = HeteroGraphConv(num_users, num_items, feature_sizes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, 7, 0.01, optimizer, device, valid_loader)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth')\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, graph, features, k=5):\n",
    "    device = next(model.parameters()).device  # 获取模型参数所在的设备\n",
    "    model.eval()\n",
    "    total_dcg = 0\n",
    "    total_ndcg = 0\n",
    "    total_recall = 0\n",
    "    total_relevant_items = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = data\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "\n",
    "            user_embeddings, pos_item_embeddings, _ = model(user_ids, pos_item_ids, graph, features)\n",
    "            _, neg_item_embeddings, _ = model(user_ids, neg_item_ids, graph, features)\n",
    "\n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "\n",
    "            scores = torch.cat((pos_scores, neg_scores))\n",
    "            labels = torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores))).to(device)  # 真实标签\n",
    "\n",
    "            # 计算分数并排序\n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            sorted_labels = labels[indices]\n",
    "\n",
    "            # 使用已定义的函数计算 DCG 和 NDCG\n",
    "            dcg_value = dcg_at_k(sorted_labels, k)\n",
    "            idcg_value = dcg_at_k(torch.ones(k).to(device), k)  # 理想情况下的排序\n",
    "            ndcg_value = dcg_value / max(idcg_value, 1e-10)  # 避免除以零\n",
    "            recall_value = sorted_labels[:k].sum() / labels.sum()\n",
    "\n",
    "            total_dcg += dcg_value\n",
    "            total_ndcg += ndcg_value\n",
    "            total_recall += recall_value\n",
    "            total_relevant_items += labels.sum()\n",
    "\n",
    "    average_dcg = total_dcg / len(test_loader)\n",
    "    average_ndcg = total_ndcg / len(test_loader)\n",
    "    average_recall = total_recall / len(test_loader)\n",
    "    print(f'Average DCG@{k}: {average_dcg:.4f}')\n",
    "    print(f'Average NDCG@{k}: {average_ndcg:.4f}')\n",
    "    print(f'Average Recall@{k}: {average_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmulti-model knowledge graph multi-graph recommendation system\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, test_loader, graph, features, k)\u001b[0m\n\u001b[0;32m     11\u001b[0m user_ids, pos_item_ids, neg_item_ids \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     12\u001b[0m user_ids, pos_item_ids, neg_item_ids \u001b[38;5;241m=\u001b[39m user_ids\u001b[38;5;241m.\u001b[39mto(device), pos_item_ids\u001b[38;5;241m.\u001b[39mto(device), neg_item_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 14\u001b[0m user_embeddings, pos_item_embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_item_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m _, neg_item_embeddings, _ \u001b[38;5;241m=\u001b[39m model(user_ids, neg_item_ids, graph, features)\n\u001b[0;32m     17\u001b[0m pos_scores \u001b[38;5;241m=\u001b[39m (user_embeddings \u001b[38;5;241m*\u001b[39m pos_item_embeddings)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36mHeteroGraphConv.forward\u001b[1;34m(self, user_ids, item_ids, graph, features)\u001b[0m\n\u001b[0;32m     49\u001b[0m item_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_embeddings(item_ids)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Process modal-specific features if provided\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovieimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     54\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_transform(features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovietext\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     55\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_transform(features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieaudio\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "features = {\n",
    "    'movieimage': torch.randn(5996, 2048),\n",
    "    'movietext': torch.randn(5996, 384),\n",
    "    'movieaudio': torch.randn(5996, 128)\n",
    "}\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth'))\n",
    "model.eval()\n",
    "test_model(model, valid_loader, graph, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, lambda_reg, optimizer, device, validation_loader=None, patience=3):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    best_ndcg = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(100):  # Set a sufficiently large number for epochs\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "            _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "            \n",
    "            # Calculate BPR Loss\n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "            \n",
    "            reg_loss = lambda_reg * (user_embeddings.norm(2).pow(2) +\n",
    "                                     pos_item_embeddings.norm(2).pow(2) +\n",
    "                                     neg_item_embeddings.norm(2).pow(2))\n",
    "            total_loss_val = loss + reg_loss\n",
    "            total_loss_val.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += total_loss_val.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        if validation_loader:\n",
    "            _, average_ndcg, _ = evaluate_metrics(model, validation_loader)  # Assuming NDCG is the second returned value\n",
    "            if average_ndcg > best_ndcg:\n",
    "                best_ndcg = average_ndcg\n",
    "                epochs_no_improve = 0\n",
    "                # Optionally save the best model\n",
    "                torch.save(model.state_dict(), 'model_best.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "                    break\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, validation_loader, k=5):\n",
    "    device = next(model.parameters()).device  # 获取模型参数所在的设备\n",
    "    model.eval()\n",
    "    total_dcg = 0\n",
    "    total_ndcg = 0\n",
    "    total_recall = 0\n",
    "    total_relevant_items = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = data\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            user_embeddings, pos_item_embeddings = model(user_ids, pos_item_ids)\n",
    "            _, neg_item_embeddings = model(user_ids, neg_item_ids)\n",
    "            \n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            \n",
    "            scores = torch.cat((pos_scores, neg_scores))\n",
    "            labels = torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores)))  # Real labels\n",
    "            \n",
    "            # Calculate scores and sort them\n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            sorted_labels = labels[indices]\n",
    "            \n",
    "            # Compute DCG and NDCG using predefined functions\n",
    "            dcg_value = dcg_at_k(sorted_labels[:k], k)\n",
    "            idcg_value = dcg_at_k(torch.ones(k).to(device), k)  # Ideal case sorting\n",
    "            ndcg_value = dcg_value / max(idcg_value, 1e-10)  # Avoid division by zero\n",
    "            recall_value = sorted_labels[:k].sum() / labels.sum()\n",
    "\n",
    "            total_dcg += dcg_value\n",
    "            total_ndcg += ndcg_value\n",
    "            total_recall += recall_value\n",
    "            total_relevant_items += labels.sum()\n",
    "\n",
    "    average_dcg = total_dcg / len(validation_loader) if len(validation_loader) > 0 else 0\n",
    "    average_ndcg = total_ndcg / len(validation_loader) if len(validation_loader) > 0 else 0\n",
    "    average_recall = total_recall / total_relevant_items if total_relevant_items > 0 else 0\n",
    "\n",
    "    print(f'Average DCG@{k}: {average_dcg:.4f}')\n",
    "    print(f'Average NDCG@{k}: {average_ndcg:.4f}')\n",
    "    print(f'Average Recall@{k}: {average_recall:.4f}')\n",
    "\n",
    "    return average_dcg, average_ndcg, average_recall\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_sizes = {\n",
    "    'movie': 2560,    # 如果电影节点存储所有多媒体特征的总和\n",
    "    'actor': 50,      # 演员的嵌入维度\n",
    "    'actress': 50,    # 女演员的嵌入维度\n",
    "    'director': 50,   # 导演的嵌入维度\n",
    "    'producer': 50,   # 制片的嵌入维度\n",
    "    'movieimage': 2048,  # 图像特征维度\n",
    "    'movietext': 384,    # 文字特征维度\n",
    "    'movieaudio': 128,   # 音频特征维度\n",
    "    'user': 100         # 假设用戶的嵌入维度，可能需要根据用户数量和系统复杂性进行调整\n",
    "}\n",
    "# 初始化和配置模型，加载数据，然后开始训练和评估\n",
    "model = HeteroGraphConv(num_users, num_items, feature_sizes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, 0.01, optimizer, device, valid_loader, 7)\n",
    "average_ndcg = evaluate_metrics(model, valid_loader)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth')\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sizes = {\n",
    "    'movie': 2560,    # 如果电影节点存储所有多媒体特征的总和\n",
    "    'actor': 50,      # 演员的嵌入维度\n",
    "    'actress': 50,    # 女演员的嵌入维度\n",
    "    'director': 50,   # 导演的嵌入维度\n",
    "    'producer': 50,   # 制片的嵌入维度\n",
    "    'movieimage': 2048,  # 图像特征维度\n",
    "    'movietext': 384,    # 文字特征维度\n",
    "    'movieaudio': 128,   # 音频特征维度\n",
    "    'user': 100         # 假设用戶的嵌入维度，可能需要根据用户数量和系统复杂性进行调整\n",
    "}\n",
    "model = HeteroGraphConv(num_users, num_items, feature_sizes)\n",
    "model.cuda()  # 将模型移至适当的设备（CPU或GPU）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lambda_reg = 0.01  # 正则化权重，根据需要调整\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth')\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = HeteroGraphConv(num_users, num_items, feature_sizes)\n",
    "model.cuda()  # 将模型移至适当的设备（CPU或GPU）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['userId'].max(), df['movieId'].max())  # 输出最大的 userId 和 movieId\n",
    "user_item_pairs = list(zip(df['userId'].values, df['movieId'].values))\n",
    "train_pairs, valid_pairs = train_test_split(user_item_pairs, test_size=0.2, random_state=42)\n",
    "try:\n",
    "    for i, (user_ids, pos_item_ids, neg_item_ids) in enumerate(train_loader):\n",
    "        print(f\"Batch {i}: {user_ids}, {pos_item_ids}, {neg_item_ids}\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError with key: {e.args[0]}\")\n",
    "    # 这里可以添加更多的调试信息，如打印有问题的批次信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "    user_ids, pos_item_ids, neg_item_ids = user_ids.cuda, pos_item_ids.cuda, neg_item_ids.cuda\n",
    "    optimizer.zero_grad()\n",
    "    # 接下来是模型的调用和损失计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_ids, pos_item_ids, neg_item_ids in train_loader:\n",
    "    print(f\"user_ids type: {user_ids.dtype}, shape: {user_ids.shape}\")\n",
    "    print(f\"pos_item_ids type: {pos_item_ids.dtype}, shape: {pos_item_ids.shape}\")\n",
    "    print(f\"neg_item_ids type: {neg_item_ids.dtype}, shape: {neg_item_ids.shape}\")\n",
    "    break  # 只查看第一批数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model')\n",
    "\n",
    "\n",
    "\n",
    "# Load the model (if needed)\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據我們的程式\n",
    "\n",
    "# 定义模型和优化器\n",
    "model = HeteroGraphConv(feature_sizes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "這樣寫是對的嗎?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFTORGPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
