{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from dgl.nn import GraphConv, GATConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                                              title  \\\n",
      "0    53519                                 Death Proof (2007)   \n",
      "1    54995                               Planet Terror (2007)   \n",
      "2    55063                                 My Winnipeg (2007)   \n",
      "3    55069  4 Months, 3 Weeks and 2 Days (4 luni, 3 saptam...   \n",
      "4    56102   Endgame: Blueprint for Global Enslavement (2007)   \n",
      "\n",
      "                                   genres     imdbId   tmdbId     tconst  \\\n",
      "0  Action|Adventure|Crime|Horror|Thriller  tt1028528   1991.0  tt1028528   \n",
      "1                    Action|Horror|Sci-Fi  tt1077258   1992.0  tt1077258   \n",
      "2                     Documentary|Fantasy  tt1093842  13241.0  tt1093842   \n",
      "3                                   Drama  tt1032846   2009.0  tt1032846   \n",
      "4                             Documentary  tt1135489  18312.0  tt1135489   \n",
      "\n",
      "                                           crew_info  \n",
      "0  actor: nm0000621, actress: nm1057928, actress:...  \n",
      "1  actress: nm0000535, actor: nm0135585, actor: n...  \n",
      "2  actor: nm0991671, actress: nm0767243, actor: n...  \n",
      "3  actress: nm1671512, actress: nm1693019, actor:...  \n",
      "4  actor: nm1093953, actor: nm1093953, actor: nm1...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取CSV檔案\n",
    "csv_file_path = r'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\data\\final_data_cleaned.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 顯示前幾行數據確認讀取是否正確\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                                        crew_parsed\n",
      "0    53519  {'actor': ['nm0000621', 'nm0000233'], 'actress...\n",
      "1    54995  {'actor': ['nm0135585', 'nm0000982', 'nm000119...\n",
      "2    55063  {'actor': ['nm0991671', 'nm0624369', 'nm241163...\n",
      "3    55069  {'actor': ['nm0412096', 'nm2308578', 'nm230470...\n",
      "4    56102  {'actor': ['nm1093953', 'nm1093953', 'nm109395...\n"
     ]
    }
   ],
   "source": [
    "def parse_crew_info(crew_info):\n",
    "    crew_dict = {\n",
    "        'actor': [],\n",
    "        'actress': [],\n",
    "        'director': [],\n",
    "        'writer': [],\n",
    "        'producer': [],\n",
    "        'composer': [],\n",
    "        'editor': [],\n",
    "        'self': []\n",
    "    }\n",
    "    for item in crew_info.split(','):\n",
    "        role, person_id = item.strip().split(': ')\n",
    "        crew_dict[role].append(person_id)\n",
    "    return crew_dict\n",
    "\n",
    "# 應用函數來解析每一行的crew_info\n",
    "df['crew_parsed'] = df['crew_info'].apply(parse_crew_info)\n",
    "print(df[['movieId', 'crew_parsed']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load user ratings\n",
    "ratings_path = r'D:\\\\CODE\\\\multi-model knowledge graph multi-graph recommendation system\\\\data\\\\cleanuser_rating.csv'\n",
    "ratings_data = pd.read_csv(ratings_path)\n",
    "\n",
    "# Load the graph\n",
    "graph_path = r'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\mainmodel\\0.消融實驗\\updated_hetero_graph03_with_V_T_A_features.pkl'\n",
    "with open(graph_path, 'rb') as f:\n",
    "    graph = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'actor': 4566, 'actress': 3701, 'composer': 2011, 'director': 4010, 'editor': 2489, 'movie': 5996, 'movieaudio': 5996, 'movieimage': 5996, 'movietext': 5996, 'producer': 3952, 'self': 1049, 'user': 12171, 'writer': 3177},\n",
      "      num_edges={('movie', 'has_actor', 'actor'): 9809, ('movie', 'has_actress', 'actress'): 8327, ('movie', 'has_audio', 'movieaudio'): 5996, ('movie', 'has_composer', 'composer'): 5923, ('movie', 'has_director', 'director'): 10163, ('movie', 'has_editor', 'editor'): 4089, ('movie', 'has_image', 'movieimage'): 5996, ('movie', 'has_producer', 'producer'): 9891, ('movie', 'has_self', 'self'): 2138, ('movie', 'has_text', 'movietext'): 5996, ('movie', 'has_writer', 'writer'): 7036, ('movie', 'similar', 'movie'): 4852, ('user', 'rates', 'movie'): 549919, ('user', 'similar', 'user'): 39895602},\n",
      "      metagraph=[('movie', 'actor', 'has_actor'), ('movie', 'actress', 'has_actress'), ('movie', 'movieaudio', 'has_audio'), ('movie', 'composer', 'has_composer'), ('movie', 'director', 'has_director'), ('movie', 'editor', 'has_editor'), ('movie', 'movieimage', 'has_image'), ('movie', 'producer', 'has_producer'), ('movie', 'self', 'has_self'), ('movie', 'movietext', 'has_text'), ('movie', 'writer', 'has_writer'), ('movie', 'movie', 'similar'), ('user', 'movie', 'rates'), ('user', 'user', 'similar')])\n",
      "Node types: ['actor', 'actress', 'composer', 'director', 'editor', 'movie', 'movieaudio', 'movieimage', 'movietext', 'producer', 'self', 'user', 'writer']\n",
      "Edge types: ['has_actor', 'has_actress', 'has_audio', 'has_composer', 'has_director', 'has_editor', 'has_image', 'has_producer', 'has_self', 'has_text', 'has_writer', 'similar', 'rates', 'similar']\n",
      "Features for movietext nodes: dict_keys(['movie_id', 'features', 'text_features'])\n",
      "Shape of features for movietext nodes: torch.Size([5996, 384])\n",
      "Features for movieimage nodes: dict_keys(['features', 'movie_id'])\n",
      "Shape of features for movieimage nodes: torch.Size([5996, 2048])\n",
      "Features for movieaudio nodes: dict_keys(['movie_id', 'features', 'audio_features'])\n",
      "Shape of features for movieaudio nodes: torch.Size([5996, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example to print the graph and its features\n",
    "# Verify the graph structure and features\n",
    "print(graph)\n",
    "print(\"Node types:\", graph.ntypes)\n",
    "print(\"Edge types:\", graph.etypes)\n",
    "\n",
    "# Check for features in specific node types\n",
    "for ntype in ['movietext', 'movieimage', 'movieaudio']:\n",
    "    if 'features' in graph.nodes[ntype].data:\n",
    "        print(f\"Features for {ntype} nodes:\", graph.nodes[ntype].data.keys())\n",
    "        print(f\"Shape of features for {ntype} nodes:\", graph.nodes[ntype].data['features'].shape)\n",
    "    else:\n",
    "        print(f\"No features found for {ntype} nodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded successfully.\n",
      "Node types: ['actor', 'actress', 'composer', 'director', 'editor', 'movie', 'movieaudio', 'movieimage', 'movietext', 'producer', 'self', 'user', 'writer']\n",
      "Edge types: ['has_actor', 'has_actress', 'has_audio', 'has_composer', 'has_director', 'has_editor', 'has_image', 'has_producer', 'has_self', 'has_text', 'has_writer', 'similar', 'rates', 'similar']\n",
      "\n",
      "Node type: actor\n",
      "Features for actor nodes: dict_keys([])\n",
      "\n",
      "Node type: actress\n",
      "Features for actress nodes: dict_keys([])\n",
      "\n",
      "Node type: composer\n",
      "Features for composer nodes: dict_keys([])\n",
      "\n",
      "Node type: director\n",
      "Features for director nodes: dict_keys([])\n",
      "\n",
      "Node type: editor\n",
      "Features for editor nodes: dict_keys([])\n",
      "\n",
      "Node type: movie\n",
      "Features for movie nodes: dict_keys(['movie_id'])\n",
      "Shape of 'movie_id' for movie nodes: torch.Size([5996])\n",
      "\n",
      "Node type: movieaudio\n",
      "Features for movieaudio nodes: dict_keys(['movie_id', 'features', 'audio_features'])\n",
      "Shape of 'movie_id' for movieaudio nodes: torch.Size([5996])\n",
      "Shape of 'features' for movieaudio nodes: torch.Size([5996, 128])\n",
      "Shape of 'audio_features' for movieaudio nodes: torch.Size([5996, 128])\n",
      "\n",
      "Node type: movieimage\n",
      "Features for movieimage nodes: dict_keys(['features', 'movie_id'])\n",
      "Shape of 'features' for movieimage nodes: torch.Size([5996, 2048])\n",
      "Shape of 'movie_id' for movieimage nodes: torch.Size([5996])\n",
      "\n",
      "Node type: movietext\n",
      "Features for movietext nodes: dict_keys(['movie_id', 'features', 'text_features'])\n",
      "Shape of 'movie_id' for movietext nodes: torch.Size([5996])\n",
      "Shape of 'features' for movietext nodes: torch.Size([5996, 384])\n",
      "Shape of 'text_features' for movietext nodes: torch.Size([5996, 384])\n",
      "\n",
      "Node type: producer\n",
      "Features for producer nodes: dict_keys([])\n",
      "\n",
      "Node type: self\n",
      "Features for self nodes: dict_keys([])\n",
      "\n",
      "Node type: user\n",
      "Features for user nodes: dict_keys(['user_id'])\n",
      "Shape of 'user_id' for user nodes: torch.Size([12171])\n",
      "\n",
      "Node type: writer\n",
      "Features for writer nodes: dict_keys([])\n",
      "Relationship ('movie', 'has_actor', 'actor'):\n",
      "  Number of edges: 9809\n",
      "  Source nodes (sample): tensor([492, 492,   0,   0, 563])\n",
      "  Destination nodes (sample): tensor([0, 1, 2, 3, 4])\n",
      "Relationship ('movie', 'has_actress', 'actress'):\n",
      "  Number of edges: 8327\n",
      "  Source nodes (sample): tensor([492, 492,   0,   0, 563])\n",
      "  Destination nodes (sample): tensor([0, 1, 2, 1, 3])\n",
      "Relationship ('movie', 'has_director', 'director'):\n",
      "  Number of edges: 10163\n",
      "  Source nodes (sample): tensor([492, 492,   0,   0, 563])\n",
      "  Destination nodes (sample): tensor([0, 1, 2, 1, 3])\n",
      "Relationship ('movie', 'has_producer', 'producer'):\n",
      "  Number of edges: 9891\n",
      "  Source nodes (sample): tensor([492, 492,   0,   0, 563])\n",
      "  Destination nodes (sample): tensor([0, 1, 2, 3, 4])\n",
      "Relationship ('movie', 'has_writer', 'writer'):\n",
      "  Number of edges: 7036\n",
      "  Source nodes (sample): tensor([563, 563, 422, 422, 822])\n",
      "  Destination nodes (sample): tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Graph loaded successfully.\")\n",
    "print(\"Node types:\", graph.ntypes)\n",
    "print(\"Edge types:\", graph.etypes)\n",
    "# Check for features in all node types\n",
    "for ntype in graph.ntypes:\n",
    "    print(f\"\\nNode type: {ntype}\")\n",
    "    print(f\"Features for {ntype} nodes:\", graph.nodes[ntype].data.keys())\n",
    "    for key in graph.nodes[ntype].data.keys():\n",
    "        print(f\"Shape of '{key}' for {ntype} nodes:\", graph.nodes[ntype].data[key].shape)\n",
    "# Check for specific relationships\n",
    "relationships = [\n",
    "    ('movie', 'has_actor', 'actor'),\n",
    "    ('movie', 'has_actress', 'actress'),\n",
    "    ('movie', 'has_director', 'director'),\n",
    "    ('movie', 'has_producer', 'producer'),\n",
    "    ('movie', 'has_writer', 'writer')\n",
    "]\n",
    "\n",
    "for rel in relationships:\n",
    "    u, v = graph.edges(etype=rel)\n",
    "    print(f\"Relationship {rel}:\")\n",
    "    print(f\"  Number of edges: {len(u)}\")\n",
    "    print(f\"  Source nodes (sample): {u[:5]}\")\n",
    "    print(f\"  Destination nodes (sample): {v[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are available in 'movietext' nodes.\n",
      "Shape of the features: torch.Size([5996, 384])\n",
      "Average of features: 0.0\n",
      "Non-zero elements in features: 0\n",
      "Incorrect dimension of features for movietext: 384\n",
      "Features are available in 'movieimage' nodes.\n",
      "Shape of the features: torch.Size([5996, 2048])\n",
      "Average of features: 0.25277090072631836\n",
      "Non-zero elements in features: 8816537\n",
      "Features are available in 'movieaudio' nodes.\n",
      "Shape of the features: torch.Size([5996, 128])\n",
      "Average of features: 0.0\n",
      "Non-zero elements in features: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Function to check features for a specific node type\n",
    "def check_features(graph, node_type):\n",
    "    if node_type in graph.ntypes:\n",
    "        if 'features' in graph.nodes[node_type].data:\n",
    "            features = graph.nodes[node_type].data['features']\n",
    "            print(f\"Features are available in '{node_type}' nodes.\")\n",
    "            print(f\"Shape of the features: {features.shape}\")\n",
    "            print(f\"Average of features: {torch.mean(features)}\")\n",
    "            print(f\"Non-zero elements in features: {torch.count_nonzero(features)}\")\n",
    "            if node_type == 'movietext' and features.shape[1] != 128:  # Adjusted expected dimension\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "            elif node_type == 'movieimage' and features.shape[1] != 2048:  # Expected dimension for image\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "            elif node_type == 'movieaudio' and features.shape[1] != 128:  # Expected dimension for audio\n",
    "                print(f\"Incorrect dimension of features for {node_type}: {features.shape[1]}\")\n",
    "        else:\n",
    "            print(f\"No features data found in '{node_type}' nodes.\")\n",
    "    else:\n",
    "        print(f\"No '{node_type}' node type found in the graph.\")\n",
    "\n",
    "\n",
    "\n",
    "# Check features for each modality\n",
    "check_features(graph, 'movietext')\n",
    "check_features(graph, 'movieimage')\n",
    "check_features(graph, 'movieaudio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp\n",
      "0       0        1     4.0  1251170120\n",
      "1       0        9     3.5  1230788571\n",
      "2       0       35     2.0  1230788649\n",
      "3       0       88     2.0  1251170520\n",
      "4       0      118     4.0  1294796033\n"
     ]
    }
   ],
   "source": [
    "# Assuming userID and movieID need to be encoded\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings_data['userId'] = user_encoder.fit_transform(ratings_data['userId'])\n",
    "ratings_data['movieId'] = movie_encoder.fit_transform(ratings_data['movieId'])\n",
    "print(ratings_data.head())\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and temp (validation + test)\n",
    "train_data, temp_data = train_test_split(ratings_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp into validation and test\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.users = torch.tensor(dataframe['userId'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(dataframe['movieId'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(dataframe['rating'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = RatingsDataset(train_data)\n",
    "validation_dataset = RatingsDataset(validation_data)\n",
    "test_dataset = RatingsDataset(test_data)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp\n",
      "0       0        1     4.0  1251170120\n",
      "1       0        9     3.5  1230788571\n",
      "2       0       35     2.0  1230788649\n",
      "3       0       88     2.0  1251170520\n",
      "4       0      118     4.0  1294796033\n",
      "Train data size: 384943\n",
      "Validation data size: 82488\n",
      "Test data size: 82488\n",
      "Users batch shape: torch.Size([64])\n",
      "Items batch shape: torch.Size([64])\n",
      "Ratings batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(ratings_data.head())\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "# 檢查第一個 batch 的形狀\n",
    "for users, items, ratings in train_loader:\n",
    "    print(f\"Users batch shape: {users.shape}\")\n",
    "    print(f\"Items batch shape: {items.shape}\")\n",
    "    print(f\"Ratings batch shape: {ratings.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(user_items, item_pool, num_negatives):\n",
    "    negative_samples = {}\n",
    "    item_pool_list = list(item_pool)  # Convert set to list for sampling\n",
    "    for user, positive_items in user_items.items():\n",
    "        negative_samples[user] = []\n",
    "        while len(negative_samples[user]) < num_negatives:\n",
    "            negative_item = np.random.choice(item_pool_list)\n",
    "            if negative_item not in positive_items:\n",
    "                negative_samples[user].append(negative_item)\n",
    "    return negative_samples\n",
    "\n",
    "# Assuming `ratings_data` from previous steps\n",
    "user_items = ratings_data.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "all_items = set(ratings_data['movieId'].unique())\n",
    "\n",
    "# Generate negative samples for each user\n",
    "num_negatives = 5  # Adjust based on how many negatives per positive\n",
    "negative_samples = get_negative_samples(user_items, all_items, num_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0: 16 positive samples, 80 negative samples\n",
      "User 1: 41 positive samples, 205 negative samples\n",
      "User 2: 24 positive samples, 120 negative samples\n",
      "User 3: 55 positive samples, 275 negative samples\n",
      "User 4: 92 positive samples, 460 negative samples\n",
      "\n",
      "Example training samples:\n",
      "User 0, Positive Item 159, Negative Item 3933\n",
      "User 0, Positive Item 159, Negative Item 3318\n",
      "User 0, Positive Item 159, Negative Item 4800\n",
      "User 0, Positive Item 159, Negative Item 4022\n",
      "User 0, Positive Item 159, Negative Item 2553\n",
      "\n",
      "Statistics of positive samples per user:\n",
      "Mean: 45.18272943883001, Median: 23.0, Min: 10, Max: 1067\n",
      "\n",
      "Statistics of negative samples per user:\n",
      "Mean: 225.91364719415003, Median: 115.0, Min: 50, Max: 5335\n"
     ]
    }
   ],
   "source": [
    "# First, let's print the number of positive and negative samples for a few users\n",
    "for user in list(user_items.keys())[:5]:  # Check the first 5 users\n",
    "    print(f\"User {user}: {len(user_items[user])} positive samples, {len(negative_samples[user])} negative samples\")\n",
    "\n",
    "# Constructing train_samples from positive and negative samples\n",
    "train_samples = []\n",
    "for user, positives in user_items.items():\n",
    "    for positive in positives:\n",
    "        for negative in negative_samples[user]:  # Ensuring there's a loop over negatives\n",
    "            train_samples.append((user, positive, negative))\n",
    "\n",
    "# Now, let's inspect the first few training samples\n",
    "print(\"\\nExample training samples:\")\n",
    "for sample in train_samples[:5]:\n",
    "    print(f\"User {sample[0]}, Positive Item {sample[1]}, Negative Item {sample[2]}\")\n",
    "\n",
    "# \n",
    "# Finally, let's check the distribution of the number of items per user\n",
    "positive_counts = [len(items) for items in user_items.values()]\n",
    "negative_counts = [len(items) for items in negative_samples.values()]\n",
    "\n",
    "print(\"\\nStatistics of positive samples per user:\")\n",
    "print(f\"Mean: {np.mean(positive_counts)}, Median: {np.median(positive_counts)}, Min: {np.min(positive_counts)}, Max: {np.max(positive_counts)}\")\n",
    "print(\"\\nStatistics of negative samples per user:\")\n",
    "print(f\"Mean: {np.mean(negative_counts)}, Median: {np.median(negative_counts)}, Min: {np.min(negative_counts)}, Max: {np.max(negative_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Convert training samples list to DataFrame\n",
    "# train_samples_df = pd.DataFrame(train_samples, columns=['User', 'PositiveItem', 'NegativeItem'])\n",
    "\n",
    "# # Save DataFrame to CSV\n",
    "# train_samples_df.to_csv('training_samples.csv', index=False)\n",
    "\n",
    "# print(\"Training samples saved to 'training_samples.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import csv  # 導入 csv 模組\n",
    "\n",
    "# def get_negative_samples(user_items, item_pool, num_negatives):\n",
    "#     negative_samples = {}\n",
    "#     for user, positive_items in user_items.items():\n",
    "#         possible_negatives = list(item_pool - positive_items)\n",
    "#         negative_samples[user] = np.random.choice(possible_negatives, num_negatives, replace=False).tolist()\n",
    "#     return negative_samples\n",
    "\n",
    "# # 假設有前面步驟的 `ratings_data`\n",
    "# user_items = ratings_data.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "# all_items = set(ratings_data['movieId'].unique())\n",
    "\n",
    "# # 為每個用戶生成負樣本\n",
    "# num_negatives = 5  # 根據每個正樣本需要的負樣本數量進行調整\n",
    "# negative_samples = get_negative_samples(user_items, all_items, num_negatives)\n",
    "\n",
    "# # 函數直接批量將樣本寫入 CSV\n",
    "# def write_samples_to_csv(user_items, negative_samples, filename='training_samples.csv'):\n",
    "#     with open(filename, 'w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['User', 'PositiveItem', 'NegativeItem'])\n",
    "#         for user, positives in user_items.items():\n",
    "#             for positive in positives:\n",
    "#                 for negative in negative_samples[user]:\n",
    "#                     writer.writerow([user, positive, negative])\n",
    "\n",
    "# # 直接將訓練樣本寫入 CSV 文件\n",
    "# write_samples_to_csv(user_items, negative_samples)\n",
    "\n",
    "# print(\"訓練樣本已保存到 'training_samples.csv'。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples loaded from 'training_samples.csv'.\n",
      "   User  PositiveItem  NegativeItem\n",
      "0     0           159          3436\n",
      "1     0           159          4175\n",
      "2     0           159          3818\n",
      "3     0           159          2636\n",
      "4     0           159          2977\n",
      "5     0             1          3436\n",
      "6     0             1          4175\n",
      "7     0             1          3818\n",
      "8     0             1          2636\n",
      "9     0             1          2977\n"
     ]
    }
   ],
   "source": [
    "# Load training samples from CSV\n",
    "train_samples_df = pd.read_csv(r'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\data\\training_samples.csv')\n",
    "print(\"Training samples loaded from 'training_samples.csv'.\")\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(train_samples_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users batch: tensor([ 9705,   805,  8431,  3248,    60,    44,  4882,  3025,  6604,  2331,\n",
      "         7035,  9015,  2679,  5673,   320,  1447,  8053,  6995,  5673, 11707,\n",
      "        10587,  6120,  7885,  6501,  4177,  4478,  7081,  3020,  4562,  3930,\n",
      "        11969,  5333,  4104,  4452, 11242, 10811,  1782,  2339,  1098,  2049,\n",
      "         4377,  3184,  5385,  9775,  3535,  2087,   958,  1913,  2691, 10698,\n",
      "          554,   953,  7144, 10969, 10846,  7513,  6011,    41,  9500, 11618,\n",
      "        11628,  6522,  1420, 10099])\n",
      "Positive items batch: tensor([ 227, 2114, 3636,  369,  903, 3715, 1755,   93,  421, 2011,  793,  279,\n",
      "         549,  793,  928, 1926, 1233,  201, 2186,   90, 1712, 1672, 1781, 1827,\n",
      "        2675, 1430,    0,  282,  195,    8,  121, 2784, 3450, 1373, 3752, 1554,\n",
      "         561,  660, 1774,  277, 2545, 3203,  686, 1040, 1737,  208, 4428,  421,\n",
      "           9,  601,  328, 1182, 5171,    3,  252, 3459,  304,  210,  901, 1627,\n",
      "        1859,  208,  372, 2520])\n",
      "Negative items batch: tensor([5412, 4946,  993,  591, 2667, 2150, 4212, 1133, 4447, 4545,  878, 2795,\n",
      "        3447, 3726, 5677, 5558, 4649, 1875,  241, 2624, 1597, 4031, 5986, 1471,\n",
      "        4317, 4281, 3395,  747, 4439, 4958, 3471, 4886, 1104,  391, 4646, 1661,\n",
      "        2543, 1778, 3454, 3716,  461, 5466, 3173, 1122, 1415, 2839, 5884, 2789,\n",
      "         816, 4581, 3341, 5987, 4530, 5724, 4000, 1044,    3, 1756, 1391, 1114,\n",
      "        2603, 3105, 4322, 1814])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.users = torch.tensor(dataframe['User'].values, dtype=torch.long)\n",
    "        self.pos_items = torch.tensor(dataframe['PositiveItem'].values, dtype=torch.long)\n",
    "        self.neg_items = torch.tensor(dataframe['NegativeItem'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.pos_items[idx], self.neg_items[idx]\n",
    "\n",
    "# Create dataset object\n",
    "train_dataset = BPRDataset(train_samples_df)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Check the first batch to verify\n",
    "for batch in train_loader:\n",
    "    users, pos_items, neg_items = batch\n",
    "    print(\"Users batch:\", users)\n",
    "    print(\"Positive items batch:\", pos_items)\n",
    "    print(\"Negative items batch:\", neg_items)\n",
    "    break  # Just check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User indices: tensor([    0,     0,     0,  ..., 12170, 12170, 12170])\n",
      "Positive item indices: tensor([159, 159, 159,  ..., 507, 507, 507])\n",
      "Negative item indices: tensor([3436, 4175, 3818,  ..., 2561, 4251, 3461])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Determine the number of unique users and items\n",
    "num_users = train_samples_df['User'].nunique()\n",
    "num_items = train_samples_df[['PositiveItem', 'NegativeItem']].nunique().sum()\n",
    "\n",
    "# Create tensors for user, positive item, and negative item indices\n",
    "user_indices = torch.tensor(train_samples_df['User'].values, dtype=torch.long)\n",
    "item_indices_pos = torch.tensor(train_samples_df['PositiveItem'].values, dtype=torch.long)\n",
    "item_indices_neg = torch.tensor(train_samples_df['NegativeItem'].values, dtype=torch.long)\n",
    "\n",
    "print(\"User indices:\", user_indices)\n",
    "print(\"Positive item indices:\", item_indices_pos)\n",
    "print(\"Negative item indices:\", item_indices_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11991\n",
      "12171\n"
     ]
    }
   ],
   "source": [
    "num_users = train_samples_df['User'].nunique()\n",
    "num_items = train_samples_df[['PositiveItem', 'NegativeItem']].nunique().sum()  # Count unique positive and negative items\n",
    "print(num_items)\n",
    "print(num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class MKGMR(nn.Module):\n",
    "    def __init__(self, num_features, num_users, num_items, hidden_dim, out_dim, num_heads, num_layers, dropout_rate=0.5):\n",
    "        super(MKGMR, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.common_dim = num_features  # Target common dimension for all modalities\n",
    "        \n",
    "        # Embeddings for users and items\n",
    "        self.user_embeddings = nn.Embedding(num_users, num_features)\n",
    "        self.item_embeddings = nn.Embedding(num_items, num_features)\n",
    "        \n",
    "        # Transformation layers for each modality\n",
    "        self.text_transform = nn.Linear(384, self.common_dim)\n",
    "        self.image_transform = nn.Linear(2048, self.common_dim)\n",
    "        self.audio_transform = nn.Linear(128, self.common_dim)\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.gat_layers.append(GATConv(self.common_dim, hidden_dim, num_heads))\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "            self.common_dim = hidden_dim * num_heads\n",
    "        self.gat_layers.append(GATConv(self.common_dim, out_dim, 1))  # The final layer with 1 head\n",
    "        self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Final projection layers\n",
    "        self.user_transform = nn.Linear(out_dim, out_dim)\n",
    "        self.item_transform = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, user_indices, item_indices_pos, item_indices_neg, text_features, image_features, audio_features):\n",
    "        # Embed users and items\n",
    "        user_emb = self.user_embeddings(user_indices)\n",
    "        item_emb_pos = self.item_embeddings(item_indices_pos)\n",
    "        item_emb_neg = self.item_embeddings(item_indices_neg)\n",
    "        \n",
    "        # Transform and aggregate multimodal features\n",
    "        text_emb = self.text_transform(text_features)\n",
    "        image_emb = self.image_transform(image_features)\n",
    "        audio_emb = self.audio_transform(audio_features)\n",
    "        \n",
    "        movie_emb = (text_emb + image_emb + audio_emb) / 3  # Simple average for aggregation\n",
    "        \n",
    "        # Apply GAT layers with dropout and symmetric normalization\n",
    "        x = torch.cat([user_emb, item_emb_pos, item_emb_neg, movie_emb], dim=0)\n",
    "        g.ndata['h'] = x\n",
    "        g = dgl.add_self_loop(g)  # Adding self-loops for normalization\n",
    "        degs = g.in_degrees().float().clamp(min=1)\n",
    "        norm = torch.pow(degs, -0.5)\n",
    "        norm = norm.to(x.device).unsqueeze(1)\n",
    "\n",
    "        for gat_layer, dropout_layer in zip(self.gat_layers, self.dropout_layers):\n",
    "            x = gat_layer(g, g.ndata['h'])\n",
    "            x = x * norm\n",
    "            x = F.relu(x)\n",
    "            x = dropout_layer(x)\n",
    "            g.ndata['h'] = x\n",
    "\n",
    "        # Separate the embeddings\n",
    "        user_emb, item_emb_pos, item_emb_neg, movie_emb = torch.split(x, [len(user_indices), len(item_indices_pos), len(item_indices_neg), len(movie_emb)], dim=0)\n",
    "\n",
    "        # Transform embeddings for final scoring\n",
    "        user_emb = self.user_transform(user_emb)\n",
    "        item_emb_pos = self.item_transform(item_emb_pos)\n",
    "        item_emb_neg = self.item_transform(item_emb_neg)\n",
    "\n",
    "        # Compute scores\n",
    "        pos_scores = torch.sum(user_emb * item_emb_pos, dim=1)\n",
    "        neg_scores = torch.sum(user_emb * item_emb_neg, dim=1)\n",
    "\n",
    "        return pos_scores, neg_scores\n",
    "\n",
    "    def bpr_loss(self, pos_scores, neg_scores):\n",
    "        # Bayesian Personalized Ranking loss\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "        return loss\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        # L2 regularization for embeddings\n",
    "        reg_loss = (self.user_embeddings.weight.norm(2) + self.item_embeddings.weight.norm(2)) * 0.01\n",
    "        return reg_loss\n",
    "\n",
    "    def training_step(self, g, user_indices, item_indices_pos, item_indices_neg, text_features, image_features, audio_features):\n",
    "        pos_scores, neg_scores = self.forward(g, user_indices, item_indices_pos, item_indices_neg, text_features, image_features, audio_features)\n",
    "        bpr_loss = self.bpr_loss(pos_scores, neg_scores)\n",
    "        reg_loss = self.regularization_loss()\n",
    "        loss = bpr_loss + reg_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "training_step() missing 3 required positional arguments: 'text_features', 'image_features', and 'audio_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m item_indices_neg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Perform a single training step\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mTypeError\u001b[0m: training_step() missing 3 required positional arguments: 'text_features', 'image_features', and 'audio_features'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of how to use the class\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy data for demonstration purposes\n",
    "    num_features = 32\n",
    "    hidden_dim = 64\n",
    "    out_dim = 32\n",
    "    num_heads = 2\n",
    "    num_layers = 3\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    model = MKGMR(num_features, num_users, num_items, hidden_dim, out_dim, num_heads, num_layers, dropout_rate)\n",
    "\n",
    "    # Dummy graph for demonstration purposes\n",
    "    # In practice, this should be the actual graph from your dataset\n",
    "    g = dgl.graph(([0, 1, 2], [1, 2, 3]))\n",
    "\n",
    "    # Dummy indices for demonstration purposes\n",
    "    user_indices = torch.tensor([0, 1, 2])\n",
    "    item_indices_pos = torch.tensor([0, 1, 2])\n",
    "    item_indices_neg = torch.tensor([3, 4, 5])\n",
    "\n",
    "    # Perform a single training step\n",
    "    loss = model.training_step(g, user_indices, item_indices_pos, item_indices_neg)\n",
    "    print(\"Training loss:\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
